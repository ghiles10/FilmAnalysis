{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=b04ce80cf439fde31d21a0f6ca76c15f636f33a22e34d6a05bd8153dcf1bae09\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/9c/aa/b1/8433fd8b1afe7eb31196cc74a42cd778bcb52636a428da079d\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session = SparkSession.builder.appName('data_film').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark_session.read.option(\"delimiter\", \"\\t\").csv('data_film.txt', header=False)\n",
    "data = data.toDF('titre', 'date', 'duree', 'type', 'note', 'nombre avis', 'avis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- titre: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- duree: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- note: string (nullable = true)\n",
      " |-- nombre avis: string (nullable = true)\n",
      " |-- avis: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable avis traitement\n",
    "data = data.withColumn(\"avis\", F.regexp_replace(F.col(\"avis\"), \"[\\['\\\"0-9]\" , \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitment de la variable type\n",
    "data = data.withColumn(\"type\", F.regexp_replace(F.col(\"type\"), \"[\\[\\]',]\", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ne garder que l'heure pour des besoin de modélisation \n",
    "data = data.withColumn('duree', F.substring(\"duree\", 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' août ',\n",
       " ' avril ',\n",
       " ' décembre ',\n",
       " ' février ',\n",
       " ' janvier ',\n",
       " ' juillet ',\n",
       " ' juin ',\n",
       " ' mai ',\n",
       " ' novembre ',\n",
       " ' octobre ',\n",
       " ' septembre '}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recuperation_mois() : \n",
    "\n",
    "    mois = []\n",
    "    for date in data.select(F.collect_list(\"date\")).first()[0] : # liste des mois \n",
    "        regex_date = re.findall(r'[^0-9-]+', date) \n",
    "        mois.append(regex_date[0]) \n",
    "\n",
    "    return set(mois)\n",
    "        \n",
    "recuperation_mois()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afin de passer du format francais a un format usuel \n",
    "data = data.withColumn(\"date\", F.regexp_replace('date','août', \"08\"))\n",
    "data = data.withColumn(\"date\", F.regexp_replace('date','avril', \"04\"))\n",
    "data = data.withColumn(\"date\", F.regexp_replace('date','décembre', \"12\"))\n",
    "data = data.withColumn(\"date\", F.regexp_replace('date','février', \"02\"))\n",
    "data = data.withColumn(\"date\", F.regexp_replace('date','janvier', \"01\"))\n",
    "data = data.withColumn(\"date\", F.regexp_replace('date','juillet', \"07\"))\n",
    "data = data.withColumn(\"date\", F.regexp_replace('date','juin', \"06\"))\n",
    "data = data.withColumn(\"date\", F.regexp_replace('date','mai', \"05\"))\n",
    "data = data.withColumn(\"date\", F.regexp_replace('date','novembre', \"11\"))\n",
    "data = data.withColumn(\"date\", F.regexp_replace('date','octobre', \"10\"))\n",
    "data = data.withColumn(\"date\", F.regexp_replace('date','septembre', \"09\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changer le format des dates  \n",
    "spark_session.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") # car ne veut pas reconnaitre le format de base \n",
    "data = data.withColumn(\"date\", F.to_date(\"date\", \"dd-MM-yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
