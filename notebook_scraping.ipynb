{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=8e3fd0f568dde6b0cdb06179ee8045c0dfc8157a4002d98fd28075827fc9a812\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/9c/aa/b1/8433fd8b1afe7eb31196cc74a42cd778bcb52636a428da079d\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 09:56:21 WARN Utils: Your hostname, codespaces-a0986a resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "22/12/08 09:56:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 09:56:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder.appName('data_film').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\nSyntax error at or near end of input(line 1, pos 6)\n\n== SQL ==\nheader\n------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark_session\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mcsv(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata_film.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mheader\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mfalse\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/sql/readwriter.py:496\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcsv\u001b[39m(\n\u001b[1;32m    424\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    425\u001b[0m     path: PathOrPaths,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m     unescapedQuoteHandling: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    459\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDataFrame\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    460\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Loads a CSV file and returns the result as a  :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \n\u001b[1;32m    462\u001b[0m \u001b[39m    This function will go through the input once to determine the input schema if\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39m    [('_c0', 'string'), ('_c1', 'string')]\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_opts(\n\u001b[1;32m    497\u001b[0m         schema\u001b[39m=\u001b[39;49mschema,\n\u001b[1;32m    498\u001b[0m         sep\u001b[39m=\u001b[39;49msep,\n\u001b[1;32m    499\u001b[0m         encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    500\u001b[0m         quote\u001b[39m=\u001b[39;49mquote,\n\u001b[1;32m    501\u001b[0m         escape\u001b[39m=\u001b[39;49mescape,\n\u001b[1;32m    502\u001b[0m         comment\u001b[39m=\u001b[39;49mcomment,\n\u001b[1;32m    503\u001b[0m         header\u001b[39m=\u001b[39;49mheader,\n\u001b[1;32m    504\u001b[0m         inferSchema\u001b[39m=\u001b[39;49minferSchema,\n\u001b[1;32m    505\u001b[0m         ignoreLeadingWhiteSpace\u001b[39m=\u001b[39;49mignoreLeadingWhiteSpace,\n\u001b[1;32m    506\u001b[0m         ignoreTrailingWhiteSpace\u001b[39m=\u001b[39;49mignoreTrailingWhiteSpace,\n\u001b[1;32m    507\u001b[0m         nullValue\u001b[39m=\u001b[39;49mnullValue,\n\u001b[1;32m    508\u001b[0m         nanValue\u001b[39m=\u001b[39;49mnanValue,\n\u001b[1;32m    509\u001b[0m         positiveInf\u001b[39m=\u001b[39;49mpositiveInf,\n\u001b[1;32m    510\u001b[0m         negativeInf\u001b[39m=\u001b[39;49mnegativeInf,\n\u001b[1;32m    511\u001b[0m         dateFormat\u001b[39m=\u001b[39;49mdateFormat,\n\u001b[1;32m    512\u001b[0m         timestampFormat\u001b[39m=\u001b[39;49mtimestampFormat,\n\u001b[1;32m    513\u001b[0m         maxColumns\u001b[39m=\u001b[39;49mmaxColumns,\n\u001b[1;32m    514\u001b[0m         maxCharsPerColumn\u001b[39m=\u001b[39;49mmaxCharsPerColumn,\n\u001b[1;32m    515\u001b[0m         maxMalformedLogPerPartition\u001b[39m=\u001b[39;49mmaxMalformedLogPerPartition,\n\u001b[1;32m    516\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    517\u001b[0m         columnNameOfCorruptRecord\u001b[39m=\u001b[39;49mcolumnNameOfCorruptRecord,\n\u001b[1;32m    518\u001b[0m         multiLine\u001b[39m=\u001b[39;49mmultiLine,\n\u001b[1;32m    519\u001b[0m         charToEscapeQuoteEscaping\u001b[39m=\u001b[39;49mcharToEscapeQuoteEscaping,\n\u001b[1;32m    520\u001b[0m         samplingRatio\u001b[39m=\u001b[39;49msamplingRatio,\n\u001b[1;32m    521\u001b[0m         enforceSchema\u001b[39m=\u001b[39;49menforceSchema,\n\u001b[1;32m    522\u001b[0m         emptyValue\u001b[39m=\u001b[39;49memptyValue,\n\u001b[1;32m    523\u001b[0m         locale\u001b[39m=\u001b[39;49mlocale,\n\u001b[1;32m    524\u001b[0m         lineSep\u001b[39m=\u001b[39;49mlineSep,\n\u001b[1;32m    525\u001b[0m         pathGlobFilter\u001b[39m=\u001b[39;49mpathGlobFilter,\n\u001b[1;32m    526\u001b[0m         recursiveFileLookup\u001b[39m=\u001b[39;49mrecursiveFileLookup,\n\u001b[1;32m    527\u001b[0m         modifiedBefore\u001b[39m=\u001b[39;49mmodifiedBefore,\n\u001b[1;32m    528\u001b[0m         modifiedAfter\u001b[39m=\u001b[39;49mmodifiedAfter,\n\u001b[1;32m    529\u001b[0m         unescapedQuoteHandling\u001b[39m=\u001b[39;49munescapedQuoteHandling,\n\u001b[1;32m    530\u001b[0m     )\n\u001b[1;32m    531\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    532\u001b[0m         path \u001b[39m=\u001b[39m [path]\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/sql/readwriter.py:50\u001b[0m, in \u001b[0;36mOptionUtils._set_opts\u001b[0;34m(self, schema, **options)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mSet named options (filter out those the value is None)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mschema(schema)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m options\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     52\u001b[0m     \u001b[39mif\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/sql/readwriter.py:118\u001b[0m, in \u001b[0;36mDataFrameReader.schema\u001b[0;34m(self, schema)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jreader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jreader\u001b[39m.\u001b[39mschema(jschema)\n\u001b[1;32m    117\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jreader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mschema(schema)\n\u001b[1;32m    119\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mschema should be StructType or string\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \nSyntax error at or near end of input(line 1, pos 6)\n\n== SQL ==\nheader\n------^^^\n"
     ]
    }
   ],
   "source": [
    "spark_session.read.csv(r'data_film.txt', 'header', 'false').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
